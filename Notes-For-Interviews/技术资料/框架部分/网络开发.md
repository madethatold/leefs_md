# 基础知识

### cookie与session

### cookie

cookie 由服务端产生并且封装在http请求头中

客户端需要支持并解析cookie并将其保存为本地文件。

因此用户可以在本地磁盘找到cookie。

浏览器检查搜有存储的cookie，

如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，

就会把cookie附在请求资源的http请求头中

##### cookie的生成

web服务器把cookie封装在http标头中，

随报文一起发送给客户端浏览器，客户端浏览器如果支持cookie就会解析cookie并将cookie保存在本地磁盘或者内存中

##### cookie的发送

浏览器会检查所有存储的cookie并将大于或者等于当前请求资源范围的cookie封装在http请求头中发送给服务器

##### cookie的内容

主要包含的内容比如：名字 值，过期时间，路径和域

路径和域一起构成了cookie的作用范围

过期时间可以设置很多种，比如可以设置为会话cookie，关闭浏览器自动清除，

正如spring 为bean设置的作用域为request，每一个request请求实例化一次bean

或者设置feild为session，每一个服务器会话（可能包含多个request请求，其实相当于长连接），实例化一次bean

1. 简介

   ​	Cookie是服务器存储在本地计算机上的小块文本，并随请求发送到同一服务器。 Web服务器使用HTTP标头将cookie发送到客户端。在客户端终端，浏览器解析cookie并将其保存为本地文件，可能保存在内存中或硬盘中。

   ​	具体来说，cookie机制使用一种在客户端维护状态的方案。它是客户端会话状态的存储机制，他需要用户打开客户端的cookie支持。 Cookie的作用是解决HTTP协议中缺少无状态缺陷的问题。

2. cookie 机制的实现

   ​	正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示

   浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript或者VBScript也可以生成cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。

3. cookie中的内容

   cookie的内容主要包括：名字，值，过期时间，路径和域。

   - 路径与域：一起构成cookie的作用范围。
   - 过期时间：若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。

### session

##### session 的创建

当需要使用session时，session被服务器创建并且生成一个与此session相关的sessionid，并将sessionid返回，可以存在cookie里

##### session的发送

客户端发送sessionid给服务端，服务端会检索该id看是否有相应的session

1. 简介

   ​	session机制是一种基于服务器端的会话保存机制。

   ​	当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个sessionId，如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个）；

   ​	如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，这个session id将被在本次响应中返回给客户端保存。可采用cookie保存这个session id，一般这个cookie的名字都是类似于SEEESIONID。

   ​	但cookie可以被人为的禁止，则必须有其他机制（如get请求重写url）以便在cookie被禁止时仍然能够把session id传递回服务器。



##### cookie 与session区别

##### 存储上的不同：

cookie只可以存储ascll字符串，但是如果要访问unicode字符或者二进制数据，要先对cookie进行编码，并且从cookie中直接访问java对象，没有相应的映射

session就不一样了，session可以存储各色数据类型，乃至一些数据结构，比如list，比如map

Session也可以直接存储java Beans 甚至任何Java类，对象等，Session可以视为一个Java容器。

##### 然后就是隐私策略

cookie要加密

session 存在服务器端，对用户隐形，所以比较安全

##### 有效期的差异

cookie可以长久保存，只要把cookie的过期时间设置一个很大的值

但是session不同，session存在时间太长会对服务器造成负担，

其实cookie和session 的差异，



有点像memocache和redis的差异，

memocache支持的数据类型少，

不能持久化

还不支持集群，只能单机



1. 不同的访问方法

   只有ASCII字符串可以存储在cookie中。如果需要访问Unicode字符或二进制数据，则需要先对其进行编码。无法在cookie中直接访问Java对象。要存储稍微复杂的信息，使用cookie会更难。

   Session可以访问任何类型的数据，包括但不限于字符串、Integer、List、Map等。 Session也可以直接存储Java Beans甚至任何Java类，对象等，使用起来非常方便。将Session视为Java容器类。

2. 不同的隐私政策

   Cookie存储在客户端阅读器中，对客户端可见。客户端上的某些程序可能会窥探、副本以更正cookie的内容。会话存储在服务器上，对客户端是透明的。不存在敏感信息泄露的风险。

   如果您选择cookie，最好不要写敏感信息，如帐户密码。最好加密像Google、Baidu这样的cookie信息，并将其提交给服务器进行解密，以确保我自己可以读取cookie中的信息。如果选择Session，则可以省去很多麻烦。无论如何，它被放置在服务器上，并且会话中的任何隐私都可以得到有效保护。

3. 有效期的差异

   使用Google的任何人都知道，如果您已登录Google，Google的登录信息将长期有效。用户每次访问时都不必再次登录，Google会永久记录用户的登录信息。要达到这个效果，使用cookies将是一个不错的选择。只需将cookie的到期时间属性设置为一个非常大的数字。由于Session依赖于名为JSESSIONID的cookie，并且Cookie JSESSIONID的到期时间默认为-1，因此只需关闭阅读器就会使Session无效，Session将无法永久完成信息。无法使用URL地址重写。此外，如果设置Session的超时时间太长，服务器将累积的Sessions越多，导致内存溢出的可能性就越大。

4. 不同的服务器压力

   会话保留在服务器端，每个用户都将生成一个会话。如果有很多并发用户，它会产生大量的Session并消耗大量内存。因此，具有高并发流量的站点（如Google、Baidu、Sina）不太可能使用Session来跟踪客户会话。

   Cookie保留在客户端上，不消耗服务器资源。如果有很多用户同时阅读，那么cookie是一个不错的选择。

5. 不同的浏览器支持

   客户端浏览器需要支持Cookie。如果客户端禁用cookie或不支持cookie，则会话跟踪将无效。

   如果客户端浏览器不支持cookie，则需要使用会话和URL地址重写。应该注意的是，Session程序中使用的所有URL都必须重写URL地址，否则会话会话跟踪将无效。

   如果客户端支持cookie，则可以在浏览器窗口和子窗口中将cookie设置为有效（将到期时间设置为-1），或者可以将cookie设置为在所有阅读器窗口中有效（设置到期时间）大于1）0的整数。但是，Session只能在此阅读器窗口及其子窗口中使用。如果两个浏览器窗口彼此不相关，则它们将使用两个不同的会话。 

6. 跨域支持的差异

   Cookie支持跨域访问。例如，如果domain属性设置为“.biaodianfu.com”，则所有以“.biaodianfu.com”为后缀的域名都可以访问cookie。跨域cookie现在通常在网络上使用，例如Google、Baidu、Sina。

   会话不支持跨域访问。会话仅在其所在的域名内有效。

# Socket知识

增加一点Socket知识

# 跨域请求

这其实是一个前端问题

### 跨域请求的由来

跨域请求只能执行get方法，不能执行post方法，但是这个意思就是，我可以get传参数

### 跨域请求的解决方式jsonp



# 负载均衡

### 什么是负载均衡

​	互联网早期，业务流量比较小并且业务逻辑比较简单，单台服务器便可以满足基本的需求；但随着互联网的发展，业务流量越来越大并且业务逻辑也越来越复杂，单台机器的性能问题以及单点问题凸显了出来，因此需要多台机器来进行性能的水平扩展以及避免单点故障。但是要如何将不同的用户的流量分发到不同的服务器上面呢？

​	早期的方法是使用DNS做负载，通过给客户端解析不同的IP地址，让客户端的流量直接到达各个服务器。但是这种方法有一个很大的缺点就是**延时性问题**，在做出调度策略改变以后，由于DNS各级节点的缓存并不会及时的在客户端生效，而且DNS负载的调度策略比较简单，无法满足业务需求，因此就出现了负载均衡。

##### 早期DNS负载均衡

![](../assets/DNS负载调度.png)

​	**客户端的流量首先会到达负载均衡服务器，由负载均衡服务器通过一定的调度算法将流量分发到不同的应用服务器**，同时负载均衡服务器也会对应用服务器做周期性的健康检查，当发现故障节点时便动态的将节点从应用服务器集群中剔除，以此来保证应用的高可用。

​	负载均衡又分为四层负载均衡和七层负载均衡。

- **四层负载均衡工作在OSI模型的传输层，主要工作是转发**。它在接收到客户端的流量以后通过修改数据包的地址信息将流量转发到应用服务器。

- **七层负载均衡工作在OSI模型的应用层，因为它需要解析应用层流量**。所以七层负载均衡在接到客户端的流量以后，还需要一个完整的TCP/IP协议栈。**七层负载均衡会与客户端建立一条完整的连接并将应用层的请求流量解析出来，再按照调度算法选择一个应用服务器，并与应用服务器建立另外一条连接将请求发送过去**，因此七层负载均衡的主要工作就是**代理**。

### 四层和七层负载均衡

##### 两者技术原理上的区别

应用层 表示层 会话层 传输层 网络层 数据链路层 物理层

四层负载均衡是在传输层面上做的负载均衡 

主要是通过报文中的目标地址和端口，对其进行改写，负载均衡服务器只是起一个类似路由器的转发功能，

TCP连接请求，最后还是和后台服务器直接建立了连接请求

七层负载均衡，主要是在

1. 四层负载均衡

   ​	**所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。**

   ​	以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。

   ​	TCP的连接建立，即三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作。

   ​	在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。

2. 七层负载均衡

   ​	所谓七层负载均衡，也称为“内容交换”，也就是主要**通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器**。

   ​	以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。

   ​	负载均衡设备在这种情况下，更类似于一个代理服务器。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。那么，为什么还需要七层负载均衡呢？

##### 应用场景的需求

​	七层应用负载的好处，是使得整个网络更"智能化", 参考我们之前的[另外一篇专门针对HTTP应用的优化的介绍](http://virtualadc.blog.51cto.com/3027116/580832)，就可以基本上了解这种方式的优势所在。

**例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术**。

七层负载均衡可以对客户端的请求和服务器的响应进行任意意义上的修改，提升了应用系统在网络层的灵活性

​	当然这只是七层应用的一个小案例，从技术原理上，**这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，极大的提升了应用系统在网络层的灵活性**。很多在后台，(例如Nginx或者Apache)上部署的功能可以前移到负载均衡设备上，例如客户请求中的Header重写，服务器响应中的关键字过滤或者内容插入等功能。

​	另外一个常常被提到功能就是**安全性**。网络中最常见的SYN Flood攻击，即黑客控制众多源客户端，使用虚假IP地址对同一目标发送SYN攻击，通常这种攻击会大量发送SYN报文，耗尽服务器上的相关资源，以达到Denial of Service ( DoS ) 的目的。

​	从技术原理上也可以看出，四层模式下这些SYN攻击都会被转发到后端的服务器上；**而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营**。另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文，例如SQL Injection等应用层面的特定攻击手段，从应用层面进一步提高系统整体安全。

​	现在的**7层负载均衡，主要还是着重于应用广泛的HTTP协议**，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。

##### 七层应用需要考虑的问题 ✅

- 是否真的必要。

  七层应用的确可以提高流量智能化，同时必不可免的带来设备配置复杂，负载均衡压力增高以及故障排查上的复杂性等问题。在设计系统时需要考虑四层七层同时应用的混杂情况。

- 是否真的可以提高安全性。

  例如SYN Flood攻击，七层模式的确将这些流量从服务器屏蔽，但负载均衡设备本身要有强大的抗DDoS能力，否则即使服务器正常而作为中枢调度的负载均衡设备故障也会导致整个应用的崩溃。

- 是否有足够的灵活度。

  七层应用的优势是可以让整个应用的流量智能化，但是负载均衡设备需要提供完善的七层功能，满足客户根据不同情况的基于应用的调度。最简单的一个考核就是能否取代后台Nginx或者Apache等服务器上的调度功能。能够提供一个七层应用开发接口的负载均衡设备，可以让客户根据需求任意设定功能，才真正有可能提供强大的灵活性和智能性。

### 负载均衡算法

##### 随机算法

- Random随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。

##### 轮询及加权轮询

- 轮询(Round Robbin)当**服务器群中各服务器的处理能力相同时，且每笔业务处理量差异不大时，最适合使用这种算法。** 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。
- 加权轮询(Weighted Round Robbin)为轮询中的每台服务器附加一定权重的算法。比如服务器1权重1，服务器2权重2，服务器3权重3，则顺序为1-2-2-3-3-3-1-2-2-3-3-3- ......

##### 最小连接及加权最小连接

- 最少连接(Least Connections)在多个服务器中，与处理连接数(会话数)最少的服务器进行通信的算法。即使在每台服务器处理能力各不相同，每笔业务处理量也不相同的情况下，也能够在一定程度上降低服务器的负载。
- 加权最少连接(Weighted Least Connection)为最少连接算法中的每台服务器附加权重的算法，该算法事先为每台服务器分配处理连接的数量，并将客户端请求转至连接数最少的服务器上。

##### 哈希算法

- 普通哈希
- 一致性哈希算法，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。

##### 

##### IP地址散列

- 通过管理发送方IP和目的地IP地址的散列，将来自同一发送方的分组(或发送至同一目的地的分组)统一转发到相同服务器的算法。当客户端有一系列业务需要处理而必须和一个服务器反复通信时，该算法能够以流(会话)为单位，保证来自相同客户端的通信能够一直在同一服务器中进行处理。

##### URL散列

- 通过管理客户端请求URL信息的散列，将发送至相同URL的请求转发至同一服务器的算法。

### 负载均衡的实现

##### DNS域名解析负载均衡（延迟）

​	利用DNS处理域名解析请求的同时进行负载均衡是另一种常用的方案。在DNS服务器中配置多个A记录，如：

- www.mysite.com IN A 114.100.80.1
- www.mysite.com IN A 114.100.80.2
- www.mysite.com IN A 114.100.80.3

​	每次域名解析请求都会根据负载均衡算法计算一个不同的IP地址返回，这样A记录中配置的多个服务器就构成一个集群，并可以实现负载均衡。
​	DNS域名解析负载均衡的优点是将负载均衡工作交给DNS，省略掉了网络管理的麻烦，缺点就是**DNS可能缓存A记录，不受网站控制**。事实上，**大型网站总是部分使用DNS域名解析，作为第一级负载均衡手段，然后再在内部做第二级负载均衡。**

​	其原理图如下：

![](../assets/DNS 域名解析负载均衡.png)

##### 数据链路层负载均衡(LVS)

​	**数据链路层负载均衡是指在通信协议的数据链路层修改mac地址进行负载均衡。**

​	这种数据传输方式又称作三角传输模式，负载均衡数据分发过程中不修改IP地址，**只修改目的的mac地址**，通过配置真实物理服务器集群所有机器虚拟IP和负载均衡服务器IP地址一样，从而达到负载均衡，这种负载均衡方式又称为直接路由方式（DR）。

​	在下图中，用户请求到达负载均衡服务器后，负载均衡服务器将请求数据的目的mac地址修改为真是WEB服务器的mac地址，并不修改数据包目标IP地址，因此数据可以正常到达目标WEB服务器，该服务器在处理完数据后可以经过网管服务器而不是负载均衡服务器直接到达用户浏览器。

​	**使用三角传输模式的链路层负载均衡是目前大型网站所使用的最广的一种负载均衡手段**。在linux平台上最好的链路层负载均衡开源产品是LVS(linux virtual server)。

![](../assets/数据链路层负载均衡.png)

##### IP负载均衡(SNAT)

​	IP负载均衡：即**在网络层通过修改请求目标地址**进行负载均衡。

​	用户请求数据包到达负载均衡服务器后，负载均衡服务器在操作系统内核进行获取网络数据包，根据负载均衡算法计算得到一台真实的WEB服务器地址，然后将数据包的IP地址修改为真实的WEB服务器地址，不需要通过用户进程处理。真实的WEB服务器处理完毕后，相应数据包回到负载均衡服务器，负载均衡服务器再将数据包源地址修改为自身的IP地址发送给用户浏览器。

​	这里的关键在于真实WEB服务器相应数据包如何返回给负载均衡服务器，一种是负载均衡服务器在修改目的IP地址的同时修改源地址，将数据包源地址改为自身的IP，即源地址转换（SNAT），另一种方案是将负载均衡服务器同时作为真实物理服务器的网关服务器，这样所有的数据都会到达负载均衡服务器。

​	IP负载均衡在内核进程完成数据分发，较反向代理均衡有更好的处理性能。但由于所有请求响应的数据包都需要经过负载均衡服务器，**因此负载均衡的网卡带宽成为系统的瓶颈**。

![](/Users/mrjiao/Documents/typora/java/assets/IP 负载均衡服务器.png)

##### HTTP重定向负载均衡(少见)

​	HTTP重定向服务器是一台普通的应用服务器，其唯一的功能就是根据用户的HTTP请求计算一台真实的服务器地址，并将真实的服务器地址写入HTTP重定向响应中（响应状态吗302）返回给浏览器，然后浏览器再自动请求真实的服务器。
​	这种负载均衡方案的优点是比较简单，缺点是浏览器需要每次请求两次服务器才能拿完成一次访问，性能较差；使用HTTP302响应码重定向，可能是搜索引擎判断为SEO作弊，降低搜索排名。重定向服务器自身的处理能力有可能成为瓶颈。因此这种方案在实际使用中并不见多。

![](../assets/http 重定向负载均衡.png)

##### 反向代理负载均衡(nginx)

​	传统代理服务器位于浏览器一端，代理浏览器将HTTP请求发送到互联网上。而反向代理服务器则位于网站机房一侧，代理网站web服务器接收http请求。

![](../assets/反向代理负载均衡.png)

​	反向代理的作用是保护网站安全，所有互联网的请求都必须经过代理服务器，相当于在web服务器和可能的网络攻击之间建立了一个屏障。

​	除此之外，代理服务器也可以配置缓存加速web请求。当用户第一次访问静态内容的时候，静态内存就被缓存在反向代理服务器上，这样当其他用户访问该静态内容时，就可以直接从反向代理服务器返回，加速web请求响应速度，减轻web服务器负载压力。

​	另外，反向代理服务器也可以实现负载均衡的功能。由于反向代理服务器转发请求在HTTP协议层面，因此也叫应用层负载均衡。优点是部署简单，缺点是可能成为系统的瓶颈。

![](../assets/反向代理负载均衡2.png)

##### 补充：什么是一致性哈希算法？

是集群的时候用的东西



# 单点登录

### 简介

​	单点登录SSO（Single Sign On）说得简单点就是在一个多系统共存的环境下，用户在一处登录后，就不用在其他系统中登录，也就是用户的一次登录能得到其他所有系统的信任。

​	单点登录在大型网站里使用得非常频繁，例如像阿里巴巴这样的网站，在网站的背后是成百上千的子系统，用户一次操作或交易可能涉及到几十个子系统的协作，如果每个子系统都需要用户认证，不仅用户会疯掉，各子系统也会为这种重复认证授权的逻辑搞疯掉。实现单点登录说到底就是要解决如何产生和存储那个信任，再就是其他系统如何验证这个信任的有效性，因此要点也就以下两个：

- 存储信任
- 验证信任

​	如果一个系统做到了开头所讲的效果，也就算单点登录，单点登录有不同的实现方式，本文就罗列我开发中所遇见过的实现方式。

### 实现方式

##### 以Cookie作为凭证媒介

​	最简单的单点登录实现方式，是使用cookie作为媒介，存放用户凭证。

​	用户登录父应用之后，应用返回一个加密的cookie，当用户访问子应用的时候，携带上这个cookie，授权应用解密cookie并进行校验，校验通过则登录当前用户。

![](../assets/单点登录使用cookie实现流程图.png)

​	不难发现以上方式把信任存储在客户端的Cookie中，这种方式很容易令人质疑：

- Cookie不安全
- 不能跨域实现免登录

​	对于第一个问题，通过加密Cookie可以保证安全性，当然这是在源代码不泄露的前提下。如果Cookie的加密算法泄露，攻击者通过伪造Cookie则可以伪造特定用户身份，这是很危险的。

​	对于第二个问题，则是硬伤。

<hr>

##### 通过[JSONP](https://www.cnblogs.com/chiangchou/p/jsonp.html)实现

​	对于跨域问题，可以使用JSONP实现（Jsonp，JSON with Padding， 是 json 的一种"使用模式"，可以让网页从别的域名（网站）那获取资料，即**跨域读取数据**）。

​	用户在父应用中登录后，跟Session匹配的Cookie会存到客户端中，当用户需要登录子应用的时候，授权应用访问父应用提供的JSONP接口，并在请求中带上父应用域名下的Cookie，父应用接收到请求，验证用户的登录状态，返回加密的信息，子应用通过解析返回来的加密信息来验证用户，如果通过验证则登录用户。

![](../assets/单点登录通过jsonp方式实现.png)

​	这种方式虽然能解决跨域问题，但是安全性其实跟把信任存储到Cookie是差不多的。如果一旦加密算法泄露了，攻击者可以在本地建立一个实现了登录接口的假冒父应用，通过绑定Host来把子应用发起的请求指向本地的假冒父应用，并作出回应。

​	因为攻击者完全可以按照加密算法来伪造响应请求，子应用接收到这个响应之后一样可以通过验证，并且登录特定用户。

<hr>

##### 通过页面重定向的方式

​	最后一种介绍的方式，是通过父应用和子应用来回重定向中进行通信，实现信息的安全传递。

​	父应用提供一个GET方式的登录接口，用户通过子应用重定向连接的方式访问这个接口，如果用户还没有登录，则返回一个的登录页面，用户输入账号密码进行登录。如果用户已经登录了，则生成加密的Token，并且重定向到子应用提供的验证Token的接口，通过解密和校验之后，子应用登录当前用户。

![](../assets/单点登录重定向实现方式.png)

​	这种方式较前面两种方式，接解决了上面两种方法暴露出来的安全性问题和跨域的问题，但是并没有前面两种方式方便。

<hr>

##### 使用独立登录系统

​	一般说来，大型应用会把授权的逻辑与用户信息的相关逻辑独立成一个应用，称为用户中心。

​	用户中心不处理业务逻辑，只是处理用户信息的管理以及授权给第三方应用。第三方应用需要登录的时候，则把用户的登录请求转发给用户中心进行处理，用户处理完毕返回凭证，第三方应用验证凭证，通过后就登录用户。

# [秒杀系统](https://blog.csdn.net/bigtree_3721/article/details/72760538)

### 秒杀业务分析

1. 正常电子商务流程

   （1）查询商品；（2）创建订单；（3）扣减库存；（4）更新订单；（5）付款；（6）卖家发货。

2. 秒杀业务的特性

   （1）低廉价格；（2）大幅推广；（3）瞬时售空；（4）一般是定时上架；（5）时间短、瞬时并发量高。

### 秒杀技术挑战

​	假设某网站秒杀活动只推出一件商品，预计会吸引1万人参加活动，也就说最大并发请求数是10000，秒杀系统需要面对的技术挑战有：

- 对现有网站业务造成冲击

  ​	秒杀活动只是网站营销的一个附加活动，这个活动具有时间短，并发访问量大的特点，如果和网站原有应用部署在一起，必然会对现有业务造成冲击，稍有不慎可能导致整个网站瘫痪。

  ​	解决方案：**将秒杀系统独立部署，甚至使用独立域名，使其与网站完全隔离。**

- 高并发下的应用、数据库负载

  ​	用户在秒杀开始前，通过不停刷新浏览器页面以保证不会错过秒杀，这些请求如果按照一般的网站应用架构，访问应用服务器、连接数据库，会对应用服务器和数据库服务器造成负载压力。

  ​	解决方案：**重新设计秒杀商品页面，不使用网站原来的商品详细页面，页面内容静态化，用户请求不需要经过应用服务。**

- 突然增加的网络及服务器带宽

  ​	假设商品页面大小200K（主要是商品图片大小），那么需要的网络和服务器带宽是2G（200K×10000），这些网络带宽是因为秒杀活动新增的，超过网站平时使用的带宽。

  ​	解决方案：**因为秒杀新增的网络带宽，必须和运营商重新购买或者租借**。为了减轻网站服务器的压力，需要将秒杀商品页面缓存在[CDN](https://www.cnblogs.com/tinywan/p/6067126.html)（Content Delivery Network，内容分发网络），同样需要和CDN服务商临时租借新增的出口带宽。

- 直接下单

  ​	秒杀的游戏规则是到了秒杀才能开始对商品下单购买，在此时间点之前，只能浏览商品信息，不能下单。而下单页面也是一个普通的URL，如果得到这个URL，不用等到秒杀开始就可以下单了。

  ​	解决方案：为了避免用户直接访问下单页面URL，需要将改URL动态化，即使秒杀系统的开发者也无法在秒杀开始前访问下单页面的URL。**办法是在下单页面URL加入由服务器端生成的随机数作为参数，在秒杀开始的时候才能得到**。

- 如何控制秒杀商品页面购买按钮的点亮

  ​	购买按钮只有在秒杀开始的时候才能点亮，在此之前是灰色的。如果该页面是动态生成的，当然可以在服务器端构造响应页面输出，控制该按钮是灰色还 是点亮，但是为了减轻服务器端负载压力，更好地利用CDN、反向代理等性能优化手段，该页面被设计为静态页面，缓存在CDN、反向代理服务器上，甚至用户浏览器上。秒杀开始时，用户刷新页面，请求根本不会到达应用服务器。

  ​	解决方案：**使用JavaScript脚本控制**，在秒杀商品静态页面中加入一个JavaScript文件引用，该JavaScript文件中包含秒杀开始标志为否；当秒杀开始的时候生成一个新的JavaScript文件（文件名保持不变，只是内容不一样），更新秒杀开始标志为是，加入下单页面的URL及随机数参数（这个随机数只会产生一个）

  ​	这个JavaScript文件非常小，即使每次浏览器刷新都访问JavaScript文件服务器也不会对服务器集群和网络带宽造成太大压力，即所有人看到的URL都是同一个，服务器端可以用redis这种分布式缓存服务器来保存随机数），并被用户浏览器加载，控制秒杀商品页面的展示。这个JavaScript文件的加载可以加上随机版本号（例如xx.js?v=32353823），这样就不会被浏览器、CDN和反向代理服务器缓存。

- 如何只允许第一个提交的订单被发送到订单子系统

  ​	由于最终能够成功秒杀到商品的用户只有一个，因此需要在用户提交订单时，检查是否已经有订单提交。如果已经有订单提交成功，则需要更新 JavaScript文件，更新秒杀开始标志为否，购买按钮变灰。事实上，由于最终能够成功提交订单的用户只有一个，为了减轻下单页面服务器的负载压力， 可以控制进入下单页面的入口，只有少数用户能进入下单页面，其他用户直接进入秒杀结束页面。

  ​	解决方案：假设下单服务器集群有10台服务器，每台服务器只接受最多10个下单请求。在还没有人提交订单成功之前，如果一台服务器已经有十单了，而有的一单都没处理，可能出现的用户体验不佳的场景是用户第一次点击购买按钮进入已结束页面，再刷新一下页面，有可能被一单都没有处理的服务器处理，进入了填写订单的页面，**可以考虑通过cookie的方式来应对，符合一致性原则。当然可以采用最少连接的负载均衡算法，出现上述情况的概率大大降低。**

- 如何进行下单前置检查

  1. 下单服务器检查本机已处理的下单请求数目：

     如果超过10条，直接返回已结束页面给用户；

     如果未超过10条，则用户可进入填写订单及确认页面；

  2. 检查全局已提交订单数目：

     已超过秒杀商品总数，返回已结束页面给用户；

     未超过秒杀商品总数，提交到子订单系统；

- 如何定时上架

  ​	该功能实现方式很多。不过目前比较好的方式是：**提前设定好商品的上架时间，用户可以在前台看到该商品，但是无法点击“立即购买”的按钮**。但是需要考虑的是，有人可以绕过前端的限制，直接通过URL的方式发起购买，这就需要在前台商品页面，以及bug页面到后端的数据库，都要进行时钟同步。越在后端控制，安全性越高。

  ​	定时秒杀的话，就要避免卖家在秒杀前对商品做编辑带来的不可预期的影响。这种特殊的变更需要多方面评估。一般禁止编辑，如需变更，可以走数据订正多的流程。

- 减库存的操作

  ​	有两种选择，一种是拍下减库存另外一种是付款减库存。目前采用的“拍下减库存”的方式，拍下就是一瞬间的事，对用户体验会好些。

- 库存会带来“超卖”的问题：售出数量多于库存数量

  ​	由于库存并发更新的问题，导致在实际库存已经不足的情况下，库存依然在减，导致卖家的商品卖得件数超过秒杀的预期。方案：采用乐观锁。

  ```mysql
  update auction_auctions set
  quantity = #inQuantity#
  where auction_id = #itemId# and quantity = #dbQuantity#
  ```

  ​	还有一种方式，会更好些，叫做尝试扣减库存，扣减库存成功才会进行下单逻辑：

  ```mysql
  update auction_auctions set 
  quantity = quantity-#count# 
  where auction_id = #itemId# and quantity >= #count#
  ```

- 秒杀器的应对

  ​	秒杀器一般下单个购买及其迅速，根据购买记录可以甄别出一部分。可以通过校验码达到一定的方法，这就要求校验码足够安全，不被破解，采用的方式有：秒杀专用验证码，电视公布验证码，秒杀答题。

### 秒杀架构原则

1. 尽量将请求拦截在系统上游

   ​	传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小。

   ​	例如：【一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0】。

2. 读多写少的常用多使用缓存

   ​	这是一个典型的读多写少的应用场景。

   ​	例如：【一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%】，非常适合使用缓存。

### 秒杀架构设计

##### 简介

​	秒杀系统为秒杀而设计，不同于一般的网购行为，参与秒杀活动的用户更关心的是如何能快速刷新商品页面，在秒杀开始的时候抢先进入下单页面，而不是商品详情等用户体验细节，因此秒杀系统的页面设计应尽可能简单。

- 商品页面中的购买按钮只有在秒杀活动开始的时候才变亮，在此之前及秒杀商品卖出后，该按钮都是灰色的，不可以点击。

- 下单表单也尽可能简单，购买数量只能是一个且不可以修改，送货地址和付款方式都使用用户默认设置，没有默认也可以不填，允许等订单提交后修改；

- 只有第一个提交的订单发送给网站的订单子系统，其余用户提交订单后只能看到秒杀结束页面。

​	要做一个这样的秒杀系统，业务会分为两个阶段，第一个阶段是秒杀开始前某个时间到秒杀开始， 这个阶段可以称之为准备阶段，用户在准备阶段等待秒杀； 第二个阶段就是秒杀开始到所有参与秒杀的用户获得秒杀结果，这个就称为秒杀阶段吧。

##### 前端层设计

​	首先要有一个展示秒杀商品的页面， 在这个页面上做一个秒杀活动开始的倒计时， 在准备阶段内用户会陆续打开这个秒杀的页面， 并且可能不停的刷新页面。

​	这里需要考虑两个问题：

1. 第一个是秒杀页面的展示

   我们知道一个html页面还是比较大的，即使做了压缩，http头和内容的大小也可能高达数十K，加上其他的css， js，图片等资源，如果同时有几千万人参与一个商品的抢购，一般机房带宽也就只有1G~10G，网络带宽就极有可能成为瓶颈，所以这个页面上各类静态资源首先应分开存放，然后放到cdn节点上分散压力，由于CDN节点遍布全国各地，能缓冲掉绝大部分的压力，而且还比机房带宽便宜。

2. 第二个是倒计时

   出于性能原因这个一般由js调用客户端本地时间，就有可能出现客户端时钟与服务器时钟不一致，另外服务器之间也是有可能出现时钟不一致。客户端与服务器时钟不一致可以采用客户端定时和服务器同步时间，这里考虑一下性能问题，用于同步时间的接口由于不涉及到后端逻辑，只需要将当前web服务器的时间发送给客户端就可以了，因此速度很快，就我以前测试的结果来看，一台标准的web服务器2W+QPS不会有问题，如果100W人同时刷，100W QPS也只需要50台web，一台硬件LB就可以了，并且web服务器群是可以很容易的横向扩展的(LB+DNS轮询)，这个接口可以只返回一小段json格式的数据，而且可以优化一下减少不必要cookie和其他http头的信息，所以数据量不会很大，一般来说网络不会成为瓶颈，即使成为瓶颈也可以考虑多机房专线连通，加智能DNS的解决方案；web服务器之间时间不同步可以采用统一时间服务器的方式，比如每隔1分钟所有参与秒杀活动的web服务器就与时间服务器做一次时间同步。

3. 浏览器层请求拦截

   （1）产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求;

   （2）JS层面，限制用户在x秒之内只能提交一次请求;

##### 站点层设计

​	前端层的请求拦截，只能拦住小白用户（不过这是99%的用户哟），高端的程序员根本不吃这一套，写个for循环，直接调用你后端的http请求，怎么整？

- 同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面

- 同一个item的查询，例如手机车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面

如此限流，又有99%的流量会被拦截在站点层。

##### 服务层设计

​	站点层的请求拦截，只能拦住普通程序员。但高级黑客，假设他控制了10w台肉鸡（并且假设买票不需要实名认证），这下uid的限制不行了吧？怎么整？

- 对于写请求：大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？对于写请求，做请求队列，每次只透过有限的写请求去数据层，如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”；
- 对于读请求，还用说么？cache来抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的；

​	如此限流，只有非常少的写请求，和非常少的读缓存miss的请求会透到数据层去，又有99.9%的请求被拦住了。

​	**具体实现细节如下**：

前端层：用户请求限制

站点层：用户请求拦截

服务层：用户请求分发

用户请求预处理：判断

（用redis实现异步队列来面对高并发的情况，或者concurrentLinkedQueue来处理，入队很快，出队加锁）

如何保持数据安全？



- 用户请求分发模块：使用Nginx或Apache将用户的请求分发到不同的机器上。

- 用户请求预处理模块：判断商品是不是还有剩余来决定是不是要处理该请求。

  经过HTTP服务器的分发后，单个服务器的负载相对低了一些，但总量依然可能很大，如果后台商品已经被秒杀完毕，那么直接给后来的请求返回秒杀失败即可，不必再进一步发送事务了，示例代码可以如下所示：

  ```java
  package seckill;
  import org.apache.http.HttpRequest;
  /**
   * 预处理阶段，把不必要的请求直接驳回，必要的请求添加到队列中进入下一阶段.
   */
  public class PreProcessor {
    	// 商品是否还有剩余
    	private static boolean reminds = true;
    	private static void forbidden() {
      	// Do something.
    	}
    
    	public static boolean checkReminds() {
      		if (reminds) {
        			// 远程检测是否还有剩余，该RPC接口应由数据库服务器提供，不必完全严格检查.
        			if (!RPC.checkReminds()) {
          				reminds = false;
        			}
      		return reminds;    
  		}
        
     
     // 每一个HTTP请求都要经过该预处理.
   	 public static void preProcess(HttpRequest request) {
      		if (checkReminds()) {
        			// 一个并发的队列
        			RequestQueue.queue.add(request);
      		} else {
        			// 如果已经没有商品了，则直接驳回请求即可.
        			forbidden();
      		}
    	}
  }
  ```

  - 并发队列的选择

    ​	Java的并发包提供了三个常用的并发队列实现，分别是：ConcurrentLinkedQueue 、 LinkedBlockingQueue 和 ArrayBlockingQueue。

    - ArrayBlockingQueue是初始容量固定的阻塞队列，我们可以用来作为数据库模块成功竞拍的队列，比如有10个商品，那么我们就设定一个10大小的数组队列。

    - ConcurrentLinkedQueue使用的是CAS原语无锁队列实现，是一个异步队列，入队的速度很快，出队进行了加锁，性能稍慢。

    - LinkedBlockingQueue也是阻塞的队列，入队和出队都用了加锁，当队空的时候线程会暂时阻塞。

    ​	由于我们的系统入队需求要远大于出队需求，一般不会出现队空的情况，所以我们可以选择ConcurrentLinkedQueue来作为我们的请求队列实现：

    ```java
    package seckill;
    import java.util.concurrent.ArrayBlockingQueue;
    import java.util.concurrent.ConcurrentLinkedQueue;
    import org.apache.http.HttpRequest;
    public class RequestQueue {
      	public static ConcurrentLinkedQueue<HttpRequest> queue = new ConcurrentLinkedQueue<HttpRequest>();
    }
    ```

    注意，为了保证性能，也可以使用redis中的list，通过lpop和rpush来实现队列的操作。

- 用户请求处理模块：把通过预处理的请求封装成事务提交给数据库，并返回是否成功。

  ```java
  package seckill;
  import org.apache.http.HttpRequest;
  public class Processor {
    	/**
       * 发送秒杀事务到数据库队列.
       */
    	public static void kill(BidInfo info) {
      		DB.bids.add(info);
    	}
  
    	public static void process() {
      		BidInfo info = new BidInfo(RequestQueue.queue.poll());
      		if (info != null) {
        			kill(info);
      		}
    	}
  }
  
  class BidInfo {
    	BidInfo(HttpRequest request) {
      		// Do something.
    	}
  }
  ```

- 数据库接口模块：该模块是数据库的唯一接口，负责与数据库交互，提供RPC接口供查询是否秒杀结束、剩余数量等信息。

  数据库主要是使用一个ArrayBlockingQueue来暂存有可能成功的用户请求。

  ```java
  package seckill;
  import java.util.concurrent.ArrayBlockingQueue;
  /**
  * DB应该是数据库的唯一接口.
  */
  public class DB {
    	public static int count = 10;
    	public static ArrayBlockingQueue<BidInfo> bids = new ArrayBlockingQueue<BidInfo>(10);
    	public static boolean checkReminds() {
        	// TODO
        	return true;
    	}
    	// 单线程操作
    	public static void bid() {
        	BidInfo info = bids.poll();
        	while (count-- > 0) {
            	// insert into table Bids values(item_id, user_id, bid_date, other)
            	// select count(id) from Bids where item_id = ?
            	// 如果数据库商品数量大约总数，则标志秒杀已完成，设置标志位reminds = false.
            	info = bids.poll();
        	}
    	}
  }
  ```

### 数据库设计

##### 基本概念

1. 单库

   单一数据库。

   ![](../assets/秒杀系统单库概念.png)

2. 分片

   分片解决的是“数据量太大”的问题，也就是通常说的“水平切分”。如下图所示：

   ![](../assets/秒杀系统数据库分片概念.png)

   一旦引入分片，势必有“数据路由”的概念，哪个数据访问哪个库。路由规则通常有3种方法：

   - 范围：range

     优点：简单，容易扩展

     缺点：各库压力不均（新号段更活跃）

   - 哈希：hash 【大部分互联网公司采用的方案二：哈希分库，哈希路由】

     优点：简单，数据均衡，负载均匀

     缺点：迁移麻烦（2库扩3库数据要迁移）

   - 路由服务：router-config-server

     优点：灵活性强，业务与路由算法解耦

     缺点：每次访问数据库前多一次查询

3. 分组

   分组解决“可用性”问题，分组通常通过主从复制的方式实现。

   ![](../assets/秒杀系统数据库分组概念.png)

   互联网公司数据库实际软件架构是：又分片，又分组（如下图）

   ![](../assets/秒杀系统数据库分组分片.png)

##### 设计思路

数据库软件架构师平时设计些什么东西呢？至少要考虑以下四点：

1. 如何保证数据可用性；

2. 如何提高数据库读性能（大部分应用读多写少，读会先成为瓶颈）；

3. 如何保证一致性；

4. 如何提高扩展性；

   

### 大并发带来的挑战 ✅

##### 接口的合理设计

​	一个秒杀或者抢购页面，通常分为2个部分，一个是静态的HTML等内容，另一个就是参与秒杀的Web后台请求接口。

​	通常静态HTML等内容，是通过CDN的部署，一般压力不大，核心瓶颈实际上在后台请求接口上。这个后端接口，必须能够支持高并发请求，同时，非常重要的一点，必须尽可能“快”，在最短的时间里返回用户的请求结果。为了实现尽可能快这一点，**接口的后端存储使用内存级别的操作会更好一点**。**仍然直接面向MySQL之类的存储是不合适的，如果有这种复杂业务的需求，都建议采用异步写入**。

​	结构图如下：

![](../assets/大并发带来的挑战_接口的合理设计.png)

​	当然，也有一些秒杀和抢购采用“滞后反馈”，就是说秒杀当下不知道结果，一段时间后才可以从页面中看到用户是否秒杀成功。但是，这种属于“偷懒”行为，同时给用户的体验也不好，容易被用户认为是“暗箱操作”。

##### 高并发的挑战：一定要“快”

​	我们通常衡量一个Web系统的吞吐率的指标是QPS（Query Per Second，每秒处理请求数），解决每秒数万次的高并发场景，这个指标非常关键。举个例子，我们假设处理一个业务请求平均响应时间为100ms，同时，系统内有20台Apache的Web服务器，配置MaxClients为500个（表示Apache的最大连接数目）。

​	那么，我们的Web系统的理论峰值QPS为（理想化的计算方式）：
$$
20*500/0.1 = 100000 （10万QPS）
$$
​	咦？我们的系统似乎很强大，1秒钟可以处理完10万的请求，5w/s的秒杀似乎是“纸老虎”哈。实际情况，当然没有这么理想。在高并发的实际场景下，机器都处于高负载的状态，在这个时候平均响应时间会被大大增加。

​	就Web服务器而言，**Apache打开了越多的连接进程，CPU需要处理的上下文切换也越多，额外增加了CPU的消耗，然后就直接导致平均响应时间增加**。因此上述的MaxClient数目，要根据CPU、内存等硬件因素综合考虑，绝对不是越多越好。可以通过Apache自带的abench来测试一下，取一个合适的值。然后，我们选择内存操作级别的存储的Redis，在高并发的状态下，存储的响应时间至关重要。网络带宽虽然也是一个因素，不过，这种请求数据包一般比较小，一般很少成为请求的瓶颈。负载均衡成为系统瓶颈的情况比较少，在这里不做讨论哈。

​	那么问题来了，假设我们的系统，在5w/s的高并发状态下，平均响应时间从100ms变为250ms（实际情况，甚至更多）：
$$
20*500/0.25 = 40000 （4万QPS）
$$
​	于是，我们的系统剩下了4w的QPS，面对5w每秒的请求，中间相差了1w。

​	然后，这才是真正的恶梦开始。举个例子，高速路口，1秒钟来5部车，每秒通过5部车，高速路口运作正常。突然，这个路口1秒钟只能通过4部车，车流量仍然依旧，结果必定出现大塞车。（5条车道忽然变成4条车道的感觉）。

​	同理，某一个秒内，20*500个可用连接进程都在满负荷工作中，却仍然有1万个新来请求，没有连接进程可用，系统陷入到异常状态也是预期之内。

​	其实在正常的非高并发的业务场景中，也有类似的情况出现，某个业务请求接口出现问题，响应时间极慢，将整个Web请求响应时间拉得很长，逐渐将Web服务器的可用连接数占满，其他正常的业务请求，无连接进程可用。

雪崩效应

​	更可怕的问题是，是用户的行为特点，系统越是不可用，用户的点击越频繁，恶性循环最终导致“雪崩”（其中一台Web机器挂了，导致流量分散到其他正常工作的机器上，再导致正常的机器也挂，然后恶性循环），将整个Web系统拖垮。

##### 重启与过载保护

​	如果系统发生“雪崩”，贸然重启服务，是无法解决问题的。最常见的现象是，启动起来后，立刻挂掉。这个时候，最好在入口层将流量拒绝，然后再将重启。如果是redis/memcache这种服务也挂了，重启的时候需要注意“预热”，并且很可能需要比较长的时间。

​	秒杀和抢购的场景，流量往往是超乎我们系统的准备和想象的。这个时候，过载保护是必要的。如果检测到系统满负载状态，拒绝请求也是一种保护措施。在前端设置过滤是最简单的方式，但是，这种做法是被用户认为“千夫所指”的行为。更合适一点的是，将过载保护设置在[CGI](https://baike.baidu.com/item/CGI/12648899?fr=aladdin)入口层，快速将客户的直接请求返回。

### 作弊的手段：进攻与防守

​	秒杀和抢购收到了“海量”的请求，实际上里面的水分是很大的。不少用户，为了“抢“到商品，会使用“刷票工具”等类型的辅助工具，帮助他们发送尽可能多的请求到服务器。还有一部分高级用户，制作强大的自动请求脚本。这种做法的理由也很简单，就是在参与秒杀和抢购的请求中，自己的请求数目占比越多，成功的概率越高。

​	这些都是属于“作弊的手段”，不过，有“进攻”就有“防守”，这是一场没有硝烟的战斗哈。

<hr>

##### 同一账号一次发出多个请求

​	部分用户通过浏览器的插件或者其他工具，在秒杀开始的时间里，以自己的账号，一次发送上百甚至更多的请求。实际上，这样的用户破坏了秒杀和抢购的公平性。

​	这种请求在某些没有做数据安全处理的系统里，也可能造成另外一种破坏，导致某些判断条件被绕过。例如一个简单的领取逻辑，先判断用户是否有参与记录，如果没有则领取成功，最后写入到参与记录中。这是个非常简单的逻辑，但是，在高并发的场景下，存在深深的漏洞。多个并发请求通过负载均衡服务器，分配到内网的多台Web服务器，它们首先向存储发送查询请求，然后，在某个请求成功写入参与记录的时间差内，其他的请求获查询到的结果都是“没有参与记录”。这里，就存在逻辑判断被绕过的风险。

![](../assets/秒杀系统同一个账户短时间内发送多个请求.png)

应对方案：

​	在程序入口处，一个账号只允许接受1个请求，其他请求过滤。不仅解决了同一个账号，发送N个请求的问题，还保证了后续的逻辑流程的安全。实现方案，可以通过Redis这种内存缓存服务，写入一个标志位（只允许1个请求写成功，结合watch的乐观锁的特性），成功写入的则可以继续参加。

​	或者，自己实现一个服务，将同一个账号的请求放入一个队列中，处理完一个，再处理下一个。

<hr>

##### 多账号一次发送多个请求

​	很多公司的账号注册功能，在发展早期几乎是没有限制的，很容易就可以注册很多个账号。因此，也导致了出现了一些特殊的工作室，通过编写自动注册脚本，积累了一大批“僵尸账号”，数量庞大，几万甚至几十万的账号不等，专门做各种刷的行为（这就是微博中的“僵尸粉“的来源）。举个例子，例如微博中有转发抽奖的活动，如果我们使用几万个“僵尸号”去混进去转发，这样就可以大大提升我们中奖的概率。

​	这种账号，使用在秒杀和抢购里，也是同一个道理。例如，iPhone官网的抢购，火车票黄牛党。

应对方案：

​	这种场景，可以通过检测指定机器IP请求频率就可以解决，如果发现某个IP请求频率很高，可以给它弹出一个验证码或者直接禁止它的请求：

1. 弹出验证码，最核心的追求，就是分辨出真实用户。因此，大家可能经常发现，网站弹出的验证码，有些是“鬼神乱舞”的样子，有时让我们根本无法看清。他们这样做的原因，其实也是为了让验证码的图片不被轻易识别，因为强大的“自动脚本”可以通过图片识别里面的字符，然后让脚本自动填写验证码。实际上，有一些非常创新的验证码，效果会比较好，例如给你一个简单问题让你回答，或者让你完成某些简单操作（例如百度贴吧的验证码）。
2. 直接禁止IP，实际上是有些粗暴的，因为有些真实用户的网络场景恰好是同一出口IP的，可能会有“误伤“。但是这一个做法简单高效，根据实际场景使用可以获得很好的效果。

##### 多账号不同IP发送不同请求

​	所谓道高一尺，魔高一丈。有进攻，就会有防守，永不休止。这些“工作室”，发现你对单机IP请求频率有控制之后，他们也针对这种场景，想出了他们的“新进攻方案”，就是不断改变IP。

![](/Users/mrjiao/Documents/typora/java/assets/秒杀系统多账号不同Ip发送不同请求.png)

​	有同学会好奇，这些随机IP服务怎么来的。有一些是某些机构自己占据一批独立IP，然后做成一个随机代理IP的服务，有偿提供给这些“工作室”使用。还有一些更为黑暗一点的，就是通过木马黑掉普通用户的电脑，这个木马也不破坏用户电脑的正常运作，只做一件事情，就是转发IP包，普通用户的电脑被变成了IP代理出口。通过这种做法，黑客就拿到了大量的独立IP，然后搭建为随机IP服务，就是为了挣钱。

应对方案：

​	说实话，这种场景下的请求，和真实用户的行为，已经基本相同了，想做分辨很困难。再做进一步的限制很容易“误伤“真实用户，这个时候，通常只能通过设置业务门槛高来限制这种请求了，或者通过账号行为的”数据挖掘“来提前清理掉它们。

​	僵尸账号也还是有一些共同特征的，例如账号很可能属于同一个号码段甚至是连号的，活跃度不高，等级低，资料不全等等。根据这些特点，适当设置参与门槛，例如限制参与秒杀的账号等级。通过这些业务手段，也是可以过滤掉一些僵尸号。

### 高并发下的数据安全

悲观锁

FIFO单线程队列的方式，其实之前在服务端端时候，已经将所有请求都封装在出队阻塞的队列里了

乐观锁，带版本号的乐观锁的实现方式



​	我们知道在多线程写入同一个文件的时候，会存现“线程安全”的问题（多个线程同时运行同一段代码，如果每次运行结果和单线程运行的结果是一样的，结果和预期相同，就是线程安全的）。如果是MySQL数据库，可以使用它自带的锁机制很好的解决问题，但是，在大规模并发的场景中，是不推荐使用MySQL的。秒杀和抢购的场景中，还有另外一个问题，就是“超发”，如果在这方面控制不慎，会产生发送过多的情况。我们也曾经听说过，某些电商搞抢购活动，买家成功拍下后，商家却不承认订单有效，拒绝发货。这里的问题，也许并不一定是商家奸诈，而是系统技术层面存在超发风险导致的。

<hr>

##### 超发的原因

​	假设某个抢购场景中，我们一共只有100个商品，在最后一刻，我们已经消耗了99个商品，仅剩最后一个。这个时候，系统发来多个并发请求，这批请求读取到的商品余量都是99个，然后都通过了这一个余量判断，最终导致超发。

<hr>

##### 悲观锁思路

​	解决线程安全的思路很多，可以从“悲观锁”的方向开始讨论。

​	悲观锁，也就是在修改数据的时候，采用锁定状态，排斥外部请求的修改。遇到加锁的状态，就必须等待。

![](../assets/秒杀系统超发悲观锁.png)

​	虽然上述的方案的确解决了线程安全的问题，但是，别忘记，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。

<hr>

##### FIFO队列思路

​	那好，那么我们稍微修改一下上面的场景，我们直接将请求放入队列中的，采用FIFO（First Input First Output，先进先出），这样的话，我们就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。

![](../assets/秒杀系统_超发_FIFO队列.png)

​	然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时候还是会大幅下降，系统还是陷入异常。

<hr>

##### 乐观锁思路

​	这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题。

​	不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。

​	有很多软件和服务都“乐观锁”功能的支持，例如Redis中的watch就是其中之一。通过这个实现，我们保证了数据的安全。







# 微信抢红包

两种常见算法：

这是实时计算的随机，

##### 二倍均值法

（将剩下的红包的金额除以当前人数）x两倍，这样每个人的期望获取金额数目 比如 100/10*2= 就是 20，这样每个人期望获得十块，比较公平，但是有个问题就是，每个人获取的金额不超过平均的两倍，是伪随机



##### 线段切割法

将红包想象成一条比较长的线段，然后要分为N段，所以就是确定N-1个切分点，我们可以随机出N-1个切分点，然后将结果按序cache到队列里，这要来一个发一个就好了

另一个是预先计算好，把分配好的金额放在队列里，这样来一个就从队列头删除一个，当然我们还要给队列加锁，保持并发安全，缺点就是比较消耗内存